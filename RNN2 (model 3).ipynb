{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a487c264-6825-44bd-8d07-12054db5c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 12:41:31.362482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, SimpleRNN, Reshape\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca7b5a2-f1f6-4fc7-ba1b-7bf6f36e5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/cse478/mgriffin13/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cse478/mgriffin13/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load training data from train.csv\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Load testing data from test.csv\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Assuming 'text' is the column name containing the text data in your CSV files\n",
    "# Clean and preprocess the text column in the training data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Remove special characters, tokenize, and remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, tokenize, and remove stopwords\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    clean_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]  # Remove non-alphabetic tokens and stopwords\n",
    "    return \" \".join(clean_tokens)  # Join tokens back into a single string\n",
    "\n",
    "# Apply the preprocessing function to the 'text' column in training and testing data\n",
    "train_data['clean_text'] = train_data['text'].apply(preprocess_text)\n",
    "test_data['clean_text'] = test_data['text'].apply(preprocess_text)\n",
    "\n",
    "# Separate features and target for training and testing\n",
    "X_train = train_data['clean_text']\n",
    "y_train = train_data['target']\n",
    "\n",
    "X_test = test_data['clean_text']\n",
    "\n",
    "# Read the predicted file from our Bayes to get a y_test\n",
    "df_test = pd.read_csv('test_predictions_cv.csv')\n",
    "df_groundtruth = pd.read_csv('ground_truth.csv')\n",
    "\n",
    "# Merge the DataFrames on 'id' column to have a single DataFrame for comparison\n",
    "merged_df = pd.merge(df_test, df_groundtruth, on='id', suffixes=('_predicted', '_groundtruth'))\n",
    "y_test = merged_df.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129ce4b-6f5f-4129-94c0-c293b1957905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 12:41:35.890171: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "238/238 [==============================] - ETA: 0s - loss: 0.5566 - accuracy: 0.7093"
     ]
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "max_words = 10000  # Define the maximum number of words in your vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences and pad sequences for uniform length\n",
    "maxlen = 100  # Define the maximum length of sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "# Create the RNN model\n",
    "embedding_dim = 50  # Define the embedding dimension\n",
    "num_classes = 2  # Assuming binary classification\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))\n",
    "model.add(SimpleRNN(64))  # Adjust the number of units as needed\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Use 'sigmoid' for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 5  # Define the number of epochs\n",
    "model.fit(X_train_pad, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_pad, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2fd40d-ca71-4f16-90f6-392dcaa37441",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06975f-011a-4448-8e04-37c0c7ec3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "# Evaluate the model on test data to get probabilities\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred_probs[:, 1] > threshold).astype(int)\n",
    "\n",
    "y_pred = np.array(y_pred_binary)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Print precision and recall\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# Classification report with precision, recall, F1-score for each class\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c66666-2340-4dea-98d4-4aaebc2a51af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spacy_env_mgriffin)",
   "language": "python",
   "name": "spacy_env_mgriffin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
